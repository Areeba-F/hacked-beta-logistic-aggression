{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Hackathon - Final",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sneer-chase-scouting-descent/hacked-beta-logistic-aggression/blob/main/ML_Hackathon_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjLXu0HN5OiZ"
      },
      "source": [
        "**Importing the necessary libraries**\n",
        "\n",
        "- Pandas: data manipulation and analysis\n",
        "- Numpy: math & equations with arrays\n",
        "- Matplotlib: Shows graphs and plots, pyplot is a module in it that allows for more graphical work. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awxJl0BIKMXl"
      },
      "source": [
        "import pandas as pandas\n",
        "import numpy as numpy\n",
        "import matplotlib.pyplot as plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2XO0UEM6WzC"
      },
      "source": [
        "**Importing the dataset**\n",
        "\n",
        "- A DataFrame is pandas' representation of a chart. To display data from a csv file into a DataFrame we use the .read_csv( ) functon.\n",
        "\n",
        "- To specify areas of chart to a variable, use the .iloc method, .iloc[start row:end row , start column: end column]. .values takes just the terms.\n",
        "\n",
        "- To use the dataset, go to the github repository and download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZNUMoCE6WHf"
      },
      "source": [
        "Data = pandas.read_csv('Rcardiotrain.csv')\n",
        "x = Data.iloc[:, :-1].values\n",
        "y = Data.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu9jShUOPW8P"
      },
      "source": [
        "**Splitting the dataset into training and testing data**\n",
        "\n",
        "We'll split the data into 4 variables - x and y training, x and y testing.\n",
        "\n",
        "test size is what percentage of the data will be used for testing. Putting any number in random state ensures that we'll always get the same ratio of numbers across the two sets.\n",
        "\n",
        "Scikit learn is a machine learning library that has function we can use to split our data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et6NYJcZPVMf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size = 0.20, random_state = 42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-B3SwG4RnTq"
      },
      "source": [
        "**Feature scaling**\n",
        "\n",
        "Feature scaling is essentially data normalization so that the model gets a better understanding of our data. This will be the done for the x values. \n",
        "\n",
        "When you call StandardScaler.fit(X_train), what it does is calculate the mean and variance from the values in X_train. Then calling .transform() will transform/standardize all of the features by subtracting the mean and dividing by the variance. For convenience, these two function calls can be done in one step using fit_transform().\n",
        "\n",
        "If we will use the fit method on our test data too, we will compute a new mean and variance that is a new scale for each variable and will let our model learn about our test data too. Thus, what we want to keep as a surprise is no longer unknown to our model and we will not get a good estimate of how our model is performing on the test (unseen) data which is the ultimate goal of building a model using machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BLUdCjVQGS5"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_training = scaler.fit_transform(x_training)\n",
        "x_testing = scaler.transform(x_testing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDY3fBzDTCoU"
      },
      "source": [
        "**Training the model with the logistic regression model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDbm2WdTTEf5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_training, y_training) # we fit/train/teach the model with our training data\n",
        "predicted_results = model.predict(x_testing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs3kPooWTouc"
      },
      "source": [
        "**Computing the accuracy of the model**\n",
        "\n",
        "70% accuracy with the Logistic regression model\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3314rATGTxy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae5722d-aafd-4dab-ee90-80bbcb1470e5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_testing, predicted_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7208571428571429"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT57G1wRUU6h"
      },
      "source": [
        "**Asking the user**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVWCdDx6T0fg"
      },
      "source": [
        "age = int(input('What is your age in days? '))\n",
        "gender = int(input('What is your gender? 1 if woman, 2 if man: '))\n",
        "height = int(input('What is your height in cm? '))\n",
        "weight = float(input('What is your weight in kg? '))\n",
        "systol = int(input('What is your systolic blood pressure? '))\n",
        "diastol = int(input('What is your diastlolic blood pressure? '))\n",
        "chol = int(input('What is your cholestrol level? 1: normal, 2: above normal, 3: well above normal- '))\n",
        "glu = int(input('What is your glucose level? 1: normal, 2: above normal, 3: well above normal- '))\n",
        "smoke = int(input('Do you smoke? 0 for no, 1 for yes: '))\n",
        "alc = int(input('Do you drink alcohol? 0 for no, 1 for yes: '))\n",
        "phys = int(input('Do you exercise regularly? 0 for no, 1 for yes: '))\n",
        "\n",
        "\n",
        "\n",
        "result = model.predict(scaler.transform([[age, gender, height, weight, systol, diastol, chol, glu, smoke, alc, phys]]))\n",
        "if result == 0:\n",
        "  print('No cardiovascular disease.')\n",
        "elif result ==1:\n",
        "  print('Cardiovascular disease.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}